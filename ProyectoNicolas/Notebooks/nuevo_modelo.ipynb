{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637cef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras import layers, models, callbacks, backend as K\n",
    "from tensorflow.keras.losses import LogCosh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9ce47",
   "metadata": {},
   "source": [
    "CONFIGURACI√ìN DE COLUMNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3249535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variables a ELIMINAR basada en el an√°lisis de Permutation Importance (< 0.05% de influencia)\n",
    "NOISE_COLS = ['Tecnologia', 'Tur', 'categoria_producto', 'semana_anio', 'g_art_id']\n",
    "\n",
    "# Las listas originales (del paso anterior)\n",
    "USER_NUMERICAL = [\n",
    "    'semana_anio', 'Tur', 'planta_id', 'seccion_id', 'maq_id', 'Pas', \n",
    "    'producto_id', 'estilo_id', 'Tal', 'Col', 'Tal_Fert', 'Col_Fert', \n",
    "    'Componentes', 'g_art_id', 'mp_id', 'Rechazo_comp', 'rechazo_flag', \n",
    "    'Tipo_2a_encoded'\n",
    "]\n",
    "\n",
    "USER_CATEGORICAL = [\n",
    "    'Tipo_TEJ', 'Tecnologia', 'C', 'categoria_producto', 'MP', 'mp_categoria'\n",
    "]\n",
    "LEAKAGE_COLS = ['Rechazo_comp', 'rechazo_flag', 'Tipo_2a_encoded']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00868062",
   "metadata": {},
   "source": [
    "Logica de Reclasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1777307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_features_final(df):\n",
    "    \"\"\"\n",
    "    Funci√≥n de reorganizaci√≥n de features que ahora incluye la eliminaci√≥n de ruido.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Eliminar Data Leakage y Ruido\n",
    "    cols_to_drop = [c for c in (LEAKAGE_COLS + NOISE_COLS) if c in df.columns]\n",
    "    if cols_to_drop:\n",
    "        print(f\"‚úÖ Eliminando ruido y leakage: {cols_to_drop}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # 2. Re-clasificaci√≥n Inteligente\n",
    "    potential_ids = ['planta_id', 'seccion_id', 'maq_id', 'producto_id', 'estilo_id', 'mp_id']\n",
    "    \n",
    "    final_embeddings = list(USER_CATEGORICAL)\n",
    "    final_numerics = []\n",
    "\n",
    "    for col in USER_NUMERICAL:\n",
    "        if col in cols_to_drop or col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        if col in potential_ids:\n",
    "            final_embeddings.append(col)\n",
    "        else:\n",
    "            final_numerics.append(col)\n",
    "            \n",
    "    # Filtro final de listas\n",
    "    final_embeddings = [c for c in final_embeddings if c in df.columns]\n",
    "    final_numerics = [c for c in final_numerics if c in df.columns]\n",
    "    \n",
    "    # Quitar duplicados entre las listas (solo por seguridad)\n",
    "    final_embeddings = list(set(final_embeddings))\n",
    "    final_numerics = list(set([c for c in final_numerics if c not in final_embeddings]))\n",
    "\n",
    "    print(f\"\\n---> Total de Variables Finales: {len(final_embeddings) + len(final_numerics)}\")\n",
    "    \n",
    "    return df, final_embeddings, final_numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85bf73c",
   "metadata": {},
   "source": [
    "Preprocesamiento Robusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e100eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test, embed_cols, num_cols):\n",
    "    input_train = {}\n",
    "    input_test = {}\n",
    "    encoders = {}\n",
    "    \n",
    "    # A. Procesar Embeddings (IDs y Categor√≠as)\n",
    "    for col in embed_cols:\n",
    "        # Convertir a string para tratar igual IDs num√©ricos y texto\n",
    "        X_train[col] = X_train[col].astype(str)\n",
    "        X_test[col] = X_test[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        # Ajustamos con train y manejamos desconocidos en test\n",
    "        train_vals = list(X_train[col].unique())\n",
    "        # Truco: a√±adimos una clase 'UNKNOWN' para valores nuevos en el futuro\n",
    "        train_vals.append('<UNK>') \n",
    "        le.fit(train_vals)\n",
    "        encoders[col] = le\n",
    "        \n",
    "        # Transformar Train\n",
    "        input_train[f\"in_{col}\"] = le.transform(X_train[col])\n",
    "        \n",
    "        # Transformar Test (Manejo seguro de valores no vistos)\n",
    "        # Si el valor no est√° en el encoder, asignamos el √≠ndice de <UNK> (el √∫ltimo)\n",
    "        unk_idx = len(le.classes_) - 1\n",
    "        input_test[f\"in_{col}\"] = X_test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else unk_idx).values\n",
    "\n",
    "    # B. Procesar Num√©ricas (Escalamiento)\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        # Ajustar solo en train\n",
    "        X_train_num = scaler.fit_transform(X_train[num_cols])\n",
    "        X_test_num = scaler.transform(X_test[num_cols])\n",
    "        \n",
    "        input_train[\"in_numerics\"] = X_train_num\n",
    "        input_test[\"in_numerics\"] = X_test_num\n",
    "        \n",
    "    return input_train, input_test, encoders, len(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362c7f1",
   "metadata": {},
   "source": [
    "MODELO: Zero-Inflated Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1260b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dynamic_model_tuned(embed_cols, encoders, n_numeric_features, learning_rate=0.0005):\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    \n",
    "    # --- 1. Capas de Embedding ---\n",
    "    for col in embed_cols:\n",
    "        n_vocab = len(encoders[col].classes_)\n",
    "        embed_dim = min(60, int(np.log2(n_vocab) * 2.5) + 1) # Aumento leve del tama√±o\n",
    "        \n",
    "        in_layer = layers.Input(shape=(1,), name=f\"in_{col}\")\n",
    "        inputs.append(in_layer)\n",
    "        \n",
    "        emb = layers.Embedding(input_dim=n_vocab, output_dim=embed_dim)(in_layer)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        embeddings.append(emb)\n",
    "        \n",
    "    # --- 2. Entrada Num√©rica ---\n",
    "    if n_numeric_features > 0:\n",
    "        num_in = layers.Input(shape=(n_numeric_features,), name=\"in_numerics\")\n",
    "        inputs.append(num_in)\n",
    "        embeddings.append(num_in)\n",
    "        \n",
    "    # --- 3. Concatenaci√≥n y Cuerpo Denso ---\n",
    "    x = layers.Concatenate()(embeddings)\n",
    "    \n",
    "    # Aumento de capacidad y ajuste de Dropout\n",
    "    x = layers.Dense(256, activation='relu')(x) # Aumento a 256\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x) # Aumento de Dropout\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # --- 4. Salida Especializada ---\n",
    "    # Usamos inicializador de bias negativo para la tendencia a cero\n",
    "    output = layers.Dense(1, activation='sigmoid', bias_initializer=tf.keras.initializers.Constant(-2.5))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    # ¬°USAMOS LOG-COSH LOSS!\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), \n",
    "                  loss=LogCosh, \n",
    "                  metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78e525",
   "metadata": {},
   "source": [
    "Ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28af29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Eliminando ruido y leakage: ['Rechazo_comp', 'rechazo_flag', 'Tipo_2a_encoded', 'Tecnologia', 'Tur', 'categoria_producto', 'semana_anio', 'g_art_id']\n",
      "\n",
      "---> Total de Variables Finales: 16\n",
      "\n",
      "Entrenando Modelo...\n",
      "Epoch 1/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0685 - rmse: 0.1655 - val_loss: 0.0113 - val_mae: 0.0682 - val_rmse: 0.1571 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - loss: 0.0113 - mae: 0.0665 - rmse: 0.1570 - val_loss: 0.0110 - val_mae: 0.0657 - val_rmse: 0.1547 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - loss: 0.0110 - mae: 0.0655 - rmse: 0.1548 - val_loss: 0.0109 - val_mae: 0.0653 - val_rmse: 0.1542 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5ms/step - loss: 0.0108 - mae: 0.0648 - rmse: 0.1534 - val_loss: 0.0108 - val_mae: 0.0660 - val_rmse: 0.1534 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 6ms/step - loss: 0.0107 - mae: 0.0643 - rmse: 0.1525 - val_loss: 0.0108 - val_mae: 0.0627 - val_rmse: 0.1537 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0639 - rmse: 0.1517 - val_loss: 0.0109 - val_mae: 0.0648 - val_rmse: 0.1537 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 6ms/step - loss: 0.0105 - mae: 0.0636 - rmse: 0.1514 - val_loss: 0.0108 - val_mae: 0.0647 - val_rmse: 0.1532 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5ms/step - loss: 0.0104 - mae: 0.0633 - rmse: 0.1506 - val_loss: 0.0109 - val_mae: 0.0660 - val_rmse: 0.1538 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 6ms/step - loss: 0.0101 - mae: 0.0623 - rmse: 0.1486 - val_loss: 0.0108 - val_mae: 0.0648 - val_rmse: 0.1532 - learning_rate: 5.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0621 - rmse: 0.1481 - val_loss: 0.0108 - val_mae: 0.0635 - val_rmse: 0.1537 - learning_rate: 5.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0618 - rmse: 0.1478 - val_loss: 0.0109 - val_mae: 0.0638 - val_rmse: 0.1538 - learning_rate: 5.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - loss: 0.0100 - mae: 0.0617 - rmse: 0.1476 - val_loss: 0.0108 - val_mae: 0.0638 - val_rmse: 0.1535 - learning_rate: 5.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6ms/step - loss: 0.0099 - mae: 0.0615 - rmse: 0.1471 - val_loss: 0.0108 - val_mae: 0.0642 - val_rmse: 0.1536 - learning_rate: 5.0000e-06\n",
      "Epoch 14/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 6ms/step - loss: 0.0100 - mae: 0.0617 - rmse: 0.1471 - val_loss: 0.0109 - val_mae: 0.0644 - val_rmse: 0.1537 - learning_rate: 5.0000e-06\n",
      "Epoch 15/50\n",
      "\u001b[1m9121/9121\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 6ms/step - loss: 0.0100 - mae: 0.0617 - rmse: 0.1471 - val_loss: 0.0108 - val_mae: 0.0644 - val_rmse: 0.1537 - learning_rate: 5.0000e-06\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "\n",
      "--- Validaci√≥n (Real vs Predicho) ---\n",
      "Real: 0.0000 | Pred: 0.0225\n",
      "Real: 1.0000 | Pred: 1.0000\n",
      "Real: 0.2826 | Pred: 0.0926\n",
      "Real: 0.0370 | Pred: 0.0551\n",
      "Real: 0.0112 | Pred: 0.0239\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/processed/dataset_cleaned.csv')\n",
    "# 1. Target y Separaci√≥n\n",
    "target_col = 'Und_2a_percentage'\n",
    "# ... (aseg√∫rate de que df y y est√°n bien definidos)\n",
    "y = df[target_col].values\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# 2. Reorganizaci√≥n Final\n",
    "X_clean, embed_cols, final_num_cols = reorganize_features_final(X)\n",
    "\n",
    "# 3. Split, Preprocesamiento y Reentrenamiento\n",
    "# (Usar las funciones preprocess_data y build_dynamic_model_tuned previamente definidas)\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=42)\n",
    "train_inputs, test_inputs, encoders, n_nums = preprocess_data(X_train_raw, X_test_raw, embed_cols, final_num_cols)\n",
    "\n",
    "model_final = build_dynamic_model_tuned(embed_cols, encoders, n_nums, learning_rate=0.0003) # LR ligeramente m√°s bajo\n",
    "\n",
    "\n",
    "\n",
    "# --- PASO E: Construir y Entrenar ---\n",
    "model = build_dynamic_model_tuned(embed_cols, encoders, n_nums)\n",
    "\n",
    "print(\"\\nEntrenando Modelo...\")\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    validation_data=(test_inputs, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(patience=4)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Predicci√≥n ---\n",
    "predictions = model.predict(test_inputs)\n",
    "# Mostrar las primeras 5 predicciones vs reales\n",
    "print(\"\\n--- Validaci√≥n (Real vs Predicho) ---\")\n",
    "for i in range(5):\n",
    "    print(f\"Real: {y_test[i]:.4f} | Pred: {predictions[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfcef4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.7844\n",
      "Mean Squared Error: 0.0235\n",
      "Root Mean Squared Error: 0.1532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b51f699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Calculando Importancia por Permutaci√≥n...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m RMSE_INDEX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Calculando Importancia por Permutaci√≥n...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m imps \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_permutation_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_dynamic_model_tuned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_to_monitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRMSE_INDEX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Usa el m√°ximo que puedas para estabilidad\u001b[39;49;00m\n\u001b[0;32m     60\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m sorted_imps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(imps\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Top 10 Variables m√°s Influyentes (Causan el mayor aumento de error) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m, in \u001b[0;36mcalculate_permutation_importance\u001b[1;34m(model, X_dict, y_true, metric_to_monitor, sample_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m y_true[indices]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 2. Evaluaci√≥n BASE (sin permutaci√≥n)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluamos en el set de prueba y tomamos el valor de la m√©trica (ej. RMSE)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(inputs_sample, y_sample, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[metric_to_monitor] \n\u001b[0;32m     26\u001b[0m importances \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 3. Iterar y Permutar\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# C√ìDIGO PARA CALCULAR IMPORTANCIA\n",
    "# ==========================================\n",
    "\n",
    "def calculate_permutation_importance(model, X_dict, y_true, metric_to_monitor, sample_size=5000):\n",
    "    \"\"\"\n",
    "    Calcula la Importancia por Permutaci√≥n usando la m√©trica del modelo (RMSE).\n",
    "    \n",
    "    Args:\n",
    "        model: El modelo Keras ya entrenado (model_tuned).\n",
    "        X_dict: Diccionario de inputs de prueba (test_inputs).\n",
    "        y_true: Valores reales de y_test.\n",
    "        metric_to_monitor: √çndice de la m√©trica a usar (0=loss, 1=mae, 2=rmse).\n",
    "        sample_size: N√∫mero de filas a usar para el c√°lculo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Seleccionar una muestra (para velocidad)\n",
    "    indices = np.random.choice(len(y_true), min(sample_size, len(y_true)), replace=False)\n",
    "    inputs_sample = {k: v[indices] for k, v in X_dict.items()}\n",
    "    y_sample = y_true[indices]\n",
    "    \n",
    "    # 2. Evaluaci√≥n BASE (sin permutaci√≥n)\n",
    "    # Evaluamos en el set de prueba y tomamos el valor de la m√©trica (ej. RMSE)\n",
    "    baseline_score = model.evaluate(inputs_sample, y_sample, verbose=0)[metric_to_monitor] \n",
    "    \n",
    "    importances = {}\n",
    "    \n",
    "    # 3. Iterar y Permutar\n",
    "    for key in inputs_sample.keys():\n",
    "        save_col = inputs_sample[key].copy()\n",
    "        \n",
    "        # PERMUTAR: Romper la relaci√≥n de la columna con el objetivo\n",
    "        np.random.shuffle(inputs_sample[key])\n",
    "        \n",
    "        # Evaluar score con la columna rota\n",
    "        shuff_score = model.evaluate(inputs_sample, y_sample, verbose=0)[metric_to_monitor]\n",
    "        \n",
    "        # Calcular el aumento de error\n",
    "        importances[key] = shuff_score - baseline_score\n",
    "        \n",
    "        # RESTAURAR la columna para el siguiente ciclo\n",
    "        inputs_sample[key] = save_col\n",
    "        \n",
    "    return importances\n",
    "\n",
    "# --- EJECUCI√ìN (Corre esto con tu modelo ya entrenado) ---\n",
    "\n",
    "# El RMSE es la m√©trica con √≠ndice 2 en la lista de m√©tricas de Keras ('mae', 'rmse')\n",
    "# Si usaste ['mae', 'rmse'], el √≠ndice es 2. Si usaste ['mae'], el √≠ndice es 1.\n",
    "# Revisa el resumen de tu modelo.\n",
    "RMSE_INDEX = 2 \n",
    "\n",
    "print(\"\\nüöÄ Calculando Importancia por Permutaci√≥n...\")\n",
    "imps = calculate_permutation_importance(\n",
    "    model=build_dynamic_model_tuned, \n",
    "    X_dict=test_inputs, \n",
    "    y_true=y_test, \n",
    "    metric_to_monitor=RMSE_INDEX, \n",
    "    sample_size=10000 # Usa el m√°ximo que puedas para estabilidad\n",
    ")\n",
    "sorted_imps = sorted(imps.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n--- Top 10 Variables m√°s Influyentes (Causan el mayor aumento de error) ---\")\n",
    "for name, imp in sorted_imps[:10]:\n",
    "    print(f\"{name.replace('in_', '')}: {imp*100:.3f}% (Aumento en el RMSE)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
